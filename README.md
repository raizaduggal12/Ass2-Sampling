# ğŸ“Š Sampling Techniques & Model Performance Analysis

## ğŸ“Œ Overview
This project analyzes the impact of different **sampling techniques** on **binary classification performance** using a **Credit Card Fraud Detection** dataset.  
Since the dataset is **highly imbalanced**, appropriate sampling and evaluation strategies are applied to ensure reliable and unbiased model performance.

---

## ğŸ“‚ Dataset
- **Dataset**: Credit Card Transactions  
- **Target Variable**: `Class`  
  - `0` â†’ Non-Fraud  
  - `1` â†’ Fraud  
- **Challenge**: Severe class imbalance  

---

## ğŸ§ª Sampling & Evaluation Techniques Used

The following techniques were implemented and analyzed:

1. **Simple Random Sampling**  
2. **Cross-Validation (Stratified K-Fold)**  
3. **Stratified Sampling**  
4. **Cluster Sampling**  
5. **Bootstrap Sampling**

> âš ï¸ **Note:** Cross-Validation is treated as an **evaluation technique**, not as a fixed dataset.

---

## ğŸ¤– Machine Learning Models

The following classification models were trained and evaluated:

- Logistic Regression  
- Decision Tree  
- Random Forest  
- Support Vector Machine (SVM)  
- Naive Bayes  

---

## ğŸ“ˆ Performance Metric
- **Accuracy (%)**
- Cross-Validation scores represent the **average accuracy across all folds**

---

## ğŸ“Š Performance Results (Accuracy %)

| Model | Random | Cross-Validation | Stratified | Cluster | Bootstrap |
|------|--------|------------------|------------|---------|-----------|
| Logistic Regression | 98.28 | 97.xx | 98.28 | 97.87 | 97.84 |
| Decision Tree | 93.97 | 95.xx | 96.55 | 97.87 | 98.71 |
| Random Forest | 98.28 | 99.xx | 98.28 | 97.87 | 100.00 |
| Support Vector Machine | 94.83 | 97.xx | 98.28 | 97.87 | 96.98 |
| Naive Bayes | 93.97 | 94.xx | 97.41 | 97.87 | 93.10 |

> *Exact values may vary slightly due to randomness in sampling and model initialization.*

---

## ğŸ“‰ Visualizations

### ğŸ“ˆ Model Accuracy Comparison (Line Plot)
![Accuracy Line Plot](images/accuracy_line_plot.png)

---

### ğŸ”¥ Accuracy Summary Table (Heatmap-style)
![Accuracy Table](images/table.png)

---

## ğŸ§  Key Observations
- **Random Forest** consistently achieved the highest accuracy and stability.
- **Bootstrap Sampling** improved performance for tree-based models.
- **Cross-Validation** provided the most reliable and unbiased performance estimate.
- **Stratified Sampling** effectively preserved class balance.
- Accuracy alone can be misleading for imbalanced datasets; sampling choice is crucial.

---

## ğŸ¯ Conclusion
This study demonstrates that **sampling strategy selection significantly impacts model performance**, particularly for imbalanced classification problems.  
Replacing systematic sampling with **cross-validation** improves robustness and ensures a fair and reliable evaluation of machine learning models.

---

## ğŸ› ï¸ Technologies Used
- Python  
- Pandas, NumPy  
- Scikit-learn  
- Matplotlib, Seaborn  

---

## ğŸ“ Repository Structure
```
Ass2/
â”œâ”€â”€ code.ipynb
â”œâ”€â”€ Creditcard_data.csv
â”œâ”€â”€ README.md
â””â”€â”€ images/
â”œâ”€â”€ accuracy_line_plot.png
â””â”€â”€ table.png
```